pip install transformers[torch] accelerate>=0.26.0

# Import libraries
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# Load the Cantonese sentiment dataset from Hugging Face
dataset = load_dataset("sepidmnorozy/Cantonese_sentiment")

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')

# Define a preprocessing function with padding
def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)

# Apply the preprocessing function to the dataset
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Choose a pre-trained model and set up training arguments
model = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3)  # Assuming three sentiment classes

training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',  # Updated line
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=3,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
)

# Train the model
trainer.train()
print(dataset['train'][0]) # Check the structure of the first entry

# Evaluate the model's performance
trainer.evaluate()

# Define a function to make predictions
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors='pt')
    outputs = model(**inputs)
    predictions = outputs.logits.argmax(dim=-1)
    return predictions

# Example prediction
print(predict_sentiment("我喜欢这个产品！"))  # "I like this product!"